{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152077e6",
   "metadata": {},
   "source": [
    "# Kapitel 10: Textanalyse historischer Daten mit NLTK (und optional spaCy/CLTK)\n",
    "\n",
    "Dieses Notebook ist als **instruktive Begleitung für Kapitel 10** gedacht und arbeitet mit **echten Korpora aus `nltk.corpus`** und wenigen, aber zusammenhängenden historischen Beispieltexten.\n",
    "\n",
    "## Lernziele (Notebook)\n",
    "- Korpusdaten mit `nltk.corpus` laden und inspizieren\n",
    "- Tokenisierung, Type–Token-Relation und Stoppwörter praktisch nachvollziehen\n",
    "- Wortfrequenzen und Collocations (Kollokationen) untersuchen\n",
    "- Ein einfaches diachrones Beispiel mit den US-Inauguralreden durchführen\n",
    "- Optional: Named Entity Recognition (NER) mit spaCy und ein kurzer Ausblick auf CLTK für Latein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545a851",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In dieser Zelle werden die benötigten Bibliotheken geladen und die wichtigsten NLTK-Korpora nachgeladen.\n",
    "\n",
    "Für die Videos kannst du hier kurz erklären:\n",
    "- Was `nltk` ist\n",
    "- Was ein \"Korpus\" ist\n",
    "- Warum wir ausgewählte Korpora (Genesis, Gutenberg, Inaugural) verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea237937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import gutenberg, genesis, inaugural\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import Text, bigrams\n",
    "\n",
    "# NLTK-Resources sicherheitshalber nachladen (idempotent)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "\n",
    "print(\"NLTK-Version:\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e204dc5",
   "metadata": {},
   "source": [
    "## 2. Überblick: Verwendete Korpora\n",
    "\n",
    "Hier verschaffst du dir (und den Studierenden) einen Überblick über einige historische Korpora, die bereits in NLTK enthalten sind:\n",
    "\n",
    "- **Genesis** (englische King-James-Version) – biblischer Text, frühneuzeitliches Englisch\n",
    "- **Gutenberg** – literarische Klassiker (z. B. Shakespeare, Austen, Melville)\n",
    "- **Inaugural** – US-Präsidenten-Antrittsreden (politischer Diskurs von 1789 bis in die Gegenwart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb603af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genesis fileids:\", genesis.fileids())\n",
    "print(\"\\nGutenberg fileids:\")\n",
    "print(gutenberg.fileids())\n",
    "print(\"\\nInaugural fileids (erste 10):\")\n",
    "print(inaugural.fileids()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f5fec",
   "metadata": {},
   "source": [
    "## 3. Fallstudie 1: Genesis (KJV) – Tokenisierung & Type–Token-Relation\n",
    "\n",
    "Wir nutzen das Korpus `genesis` (King James Version) als Beispiel für einen **kohärenten historischen Text**.\n",
    "\n",
    "Schritte:\n",
    "1. Rohtext ansehen (Ausschnitt)\n",
    "2. Tokenisierung\n",
    "3. Zählen von Tokens und Types\n",
    "4. Type–Token-Relation (TTR) berechnen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rohtext aus dem Genesis-Korpus (englische KJV)\n",
    "text_genesis = genesis.raw('english-kjv.txt')\n",
    "\n",
    "print(\"Ausschnitt (erste 500 Zeichen):\\n\")\n",
    "print(text_genesis[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1825e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenisierung\n",
    "tokens_genesis = word_tokenize(text_genesis)\n",
    "\n",
    "n_tokens = len(tokens_genesis)\n",
    "n_types = len(set(w.lower() for w in tokens_genesis))\n",
    "ttr = n_types / n_tokens\n",
    "\n",
    "print(\"Anzahl Tokens:\", n_tokens)\n",
    "print(\"Anzahl Types:\", n_types)\n",
    "print(\"Type–Token-Relation (TTR):\", round(ttr, 4))\n",
    "\n",
    "print(\"\\nDie ersten 30 Tokens:\")\n",
    "print(tokens_genesis[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1545a0f",
   "metadata": {},
   "source": [
    "## 4. Stoppwörter & Wortfrequenzanalyse\n",
    "\n",
    "Nun betrachten wir die häufigsten Wörter in Genesis.\n",
    "\n",
    "Schritte:\n",
    "1. Frequenzverteilung **mit** Stoppwörtern\n",
    "2. Frequenzverteilung **ohne** Stoppwörter\n",
    "3. Plot der häufigsten Wörter (log-Skala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequenzen MIT Stoppwörtern\n",
    "fd_all = FreqDist(w.lower() for w in tokens_genesis)\n",
    "print(\"Häufigste 20 Tokens (mit Stoppwörtern):\")\n",
    "print(fd_all.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequenzen OHNE Stoppwörter\n",
    "en_stop = set(stopwords.words('english'))\n",
    "tokens_genesis_nostop = [w.lower() for w in tokens_genesis if w.isalpha() and w.lower() not in en_stop]\n",
    "fd_nostop = FreqDist(tokens_genesis_nostop)\n",
    "\n",
    "print(\"Häufigste 20 Tokens (ohne Stoppwörter):\")\n",
    "print(fd_nostop.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der 30 häufigsten Wörter (ohne Stoppwörter) in log-Skala\n",
    "plt.figure(figsize=(10,4))\n",
    "fd_nostop.plot(30, cumulative=False)\n",
    "plt.yscale('log')\n",
    "plt.title('Häufigste Wörter in Genesis (ohne Stoppwörter, log-Skala)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495a68d",
   "metadata": {},
   "source": [
    "## 5. Kollokationen in Genesis\n",
    "\n",
    "Als nächstes suchen wir nach **Kollokationen**, also Wörtern, die häufig zusammen auftreten. Das ist besonders spannend für historische Fragestellungen:\n",
    "- Welche Begriffe treten gemeinsam mit *God* auf?\n",
    "- Welche typischen Formulierungen verwendet ein Text?\n",
    "\n",
    "Wir verwenden dafür `nltk.Text` und die eingebaute Methode `.collocations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_obj_genesis = Text(tokens_genesis)\n",
    "\n",
    "print(\"Kollokationen in Genesis (Top 20):\")\n",
    "text_obj_genesis.collocations(num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3dce0",
   "metadata": {},
   "source": [
    "## 6. Fallstudie 2: Politischer Diskurs – Inauguralreden & das Wort „freedom“\n",
    "\n",
    "Nun wechseln wir zu einem explizit politischen Korpus: den **US-Präsidenten-Antrittsreden** (`inaugural`).\n",
    "\n",
    "Wir betrachten als einfache diachrone Fragestellung:\n",
    "- Wie häufig kommt das Wort **„freedom“** in den verschiedenen Reden vor (relativ zur Länge der Rede)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = inaugural.fileids()\n",
    "\n",
    "data = []  # (jahr, president, rel_freq)\n",
    "\n",
    "for fid in fileids:\n",
    "    # Dateiname: 'YYYY-PresName.txt'\n",
    "    year_str, pres = fid.split('-')[0], '-'.join(fid.split('-')[1:]).replace('.txt', '')\n",
    "    year = int(year_str)\n",
    "    words = [w.lower() for w in inaugural.words(fid)]\n",
    "    total = len(words)\n",
    "    freq = words.count('freedom') / total if total > 0 else 0\n",
    "    data.append((year, pres, freq))\n",
    "\n",
    "data_sorted = sorted(data, key=lambda x: x[0])\n",
    "\n",
    "for year, pres, freq in data_sorted[:10]:\n",
    "    print(f\"{year}: {pres:20s} -> rel. freq 'freedom': {freq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der relativen Häufigkeit von 'freedom' über die Zeit\n",
    "years = [d[0] for d in data_sorted]\n",
    "freqs = [d[2] for d in data_sorted]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(years, freqs, marker='o')\n",
    "plt.xlabel('Jahr der Inauguralrede')\n",
    "plt.ylabel(\"Relative Häufigkeit von 'freedom'\")\n",
    "plt.title(\"'freedom' in US-Inauguralreden\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b5247",
   "metadata": {},
   "source": [
    "## 7. Optional: Named Entity Recognition (NER) mit spaCy\n",
    "\n",
    "Für eine **erste Demonstration von NER** kannst du `spaCy` auf eine einzelne Inauguralrede anwenden.\n",
    "\n",
    "Wir nehmen z. B. die erste Rede von Barack Obama (2009). Wir können anhand der Eigennamenerkennung:\n",
    "- zeigen, welche Personen, Orte, Organisationen im Text vorkommen\n",
    "- den Link schlagen zur historischen Netzwerkanalyse, Ortsregistern etc.\n",
    "\n",
    "Wenn `spaCy` oder das Modell nicht installiert ist, überspringt die Zelle den Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    spacy_available = True\n",
    "except Exception as e:\n",
    "    print(\"spaCy oder das englische Modell sind nicht verfügbar:\", e)\n",
    "    spacy_available = False\n",
    "\n",
    "if spacy_available:\n",
    "    obama_text = inaugural.raw('2009-Obama.txt')\n",
    "    print(\"Ausschnitt aus der Inauguralrede 2009 (erste 500 Zeichen):\\n\")\n",
    "    print(obama_text[:500])\n",
    "\n",
    "    doc = nlp_en(obama_text)\n",
    "    print(\"\\nGefundene Entitäten (erste 30):\")\n",
    "    for ent in list(doc.ents)[:30]:\n",
    "        print(ent.text, \"->\", ent.label_)\n",
    "else:\n",
    "    print(\"NER-Beispiel mit spaCy wird übersprungen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001d184",
   "metadata": {},
   "source": [
    "## 8. Optional: Kurzer Ausblick – Latein mit CLTK\n",
    "\n",
    "Zum Schluss ein **kurzer Ausblick auf CLTK** mit einem zusammenhängenden lateinischen Text.\n",
    "\n",
    "Wir verwenden einen kurzen Auszug aus Caesars *Bellum Gallicum* (1,1):\n",
    "\n",
    "> *Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.*\n",
    "\n",
    "Wir zeigen nur: Tokenisierung & Lemmatisierung.\n",
    "Wenn CLTK nicht installiert ist, wird das Beispiel übersprungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ec4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from cltk import NLP as CLTK_NLP\n",
    "    cltk_available = True\n",
    "except Exception as e:\n",
    "    print(\"CLTK ist nicht verfügbar:\", e)\n",
    "    cltk_available = False\n",
    "\n",
    "latin_caesar = (\n",
    "    \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, \"\n",
    "    \"aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\"\n",
    ")\n",
    "\n",
    "print(latin_caesar)\n",
    "\n",
    "if cltk_available:\n",
    "    nlp_lat = CLTK_NLP(\"lat\")\n",
    "    doc_lat = nlp_lat(latin_caesar)\n",
    "\n",
    "    print(\"\\nTokens:\")\n",
    "    print(doc_lat.tokens)\n",
    "\n",
    "    print(\"\\nLemmata:\")\n",
    "    print(doc_lat.lemmata)\n",
    "else:\n",
    "    print(\"Latein-Beispiel mit CLTK wird übersprungen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b68cd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Zusammenfassung & Transfer\n",
    "\n",
    "In diesem Notebook hast du anhand **kohärenter historischer Texte** gesehen, wie man:\n",
    "\n",
    "- Korpora aus `nltk.corpus` lädt und inspiziert\n",
    "- Tokenisierung, Type–Token-Relation und Stoppwörter praktisch nutzt\n",
    "- Wortfrequenzen und Kollokationen in einem biblischen Text (Genesis) analysiert\n",
    "- eine einfache diachrone Untersuchung in politischen Reden (Inauguralreden) durchführt\n",
    "- optional NER mit spaCy und lateinische Analysen mit CLTK andeutet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469931c4-6f78-4144-9337-1de174895e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
